{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sqlite3\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from keras.models import Model, Input, load_model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear_defects = [0, 35, 76, 77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size\n",
    "size = [300, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7327589988708496\n"
     ]
    }
   ],
   "source": [
    "# Load pre-processed dataset\n",
    "X_train = np.load('gears_train_300x400_0,76,77,35.npy')\n",
    "X_test = np.load('gears_test_300x400_0,76,77,35.npy')\n",
    "y_train = np.load('gears_ytrain_300x400_0,76,77,35.npy')\n",
    "y_test = np.load('gears_ytest_300x400_0,76,77,35.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           [(None, 300, 400, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 300, 400, 64) 3200        input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 300, 400, 64) 1200        conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 300, 400, 64) 0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling2D) (None, 149, 199, 64) 0           activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 149, 199, 64) 0           max_pooling2d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 149, 199, 64) 0           max_pooling2d_40[0][0]           \n",
      "                                                                 activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 149, 199, 64) 0           add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 149, 199, 64) 596         dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 149, 199, 64) 0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 149, 199, 64) 0           activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 149, 199, 64) 0           activation_237[0][0]             \n",
      "                                                                 activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 149, 199, 64) 0           add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 149, 199, 64) 596         dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 149, 199, 64) 0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 149, 199, 64) 0           activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 149, 199, 64) 0           activation_241[0][0]             \n",
      "                                                                 activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling2D) (None, 74, 99, 64)   0           add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 74, 99, 64)   0           max_pooling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 74, 99, 64)   296         dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 74, 99, 64)   0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 74, 99, 64)   0           activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 74, 99, 64)   0           activation_245[0][0]             \n",
      "                                                                 activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 74, 99, 64)   0           add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 74, 99, 64)   296         dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 74, 99, 64)   0           batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 74, 99, 64)   0           activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 74, 99, 64)   0           activation_249[0][0]             \n",
      "                                                                 activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 74, 99, 64)   0           add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, 74, 99, 64)   296         dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 74, 99, 64)   0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 74, 99, 64)   0           activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 74, 99, 64)   0           activation_253[0][0]             \n",
      "                                                                 activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 74, 99, 64)   0           add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, 74, 99, 64)   296         dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 74, 99, 64)   0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 74, 99, 64)   0           activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, 74, 99, 64)   0           activation_257[0][0]             \n",
      "                                                                 activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling2D) (None, 36, 49, 64)   0           add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 36, 49, 64)   0           max_pooling2d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchN (None, 36, 49, 64)   144         dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 36, 49, 64)   0           batch_normalization_260[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, 36, 49, 64)   0           activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_86 (Add)                    (None, 36, 49, 64)   0           activation_261[0][0]             \n",
      "                                                                 activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 36, 49, 64)   0           add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchN (None, 36, 49, 64)   144         dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, 36, 49, 64)   0           batch_normalization_264[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, 36, 49, 64)   0           activation_265[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_87 (Add)                    (None, 36, 49, 64)   0           activation_265[0][0]             \n",
      "                                                                 activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 36, 49, 64)   0           add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchN (None, 36, 49, 64)   144         dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, 36, 49, 64)   0           batch_normalization_268[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, 36, 49, 64)   0           activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_88 (Add)                    (None, 36, 49, 64)   0           activation_269[0][0]             \n",
      "                                                                 activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 36, 49, 64)   0           add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchN (None, 36, 49, 64)   144         dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, 36, 49, 64)   0           batch_normalization_272[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, 36, 49, 64)   0           activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_89 (Add)                    (None, 36, 49, 64)   0           activation_273[0][0]             \n",
      "                                                                 activation_276[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 36, 49, 64)   0           add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchN (None, 36, 49, 64)   144         dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, 36, 49, 64)   0           batch_normalization_276[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, 36, 49, 64)   0           activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_90 (Add)                    (None, 36, 49, 64)   0           activation_277[0][0]             \n",
      "                                                                 activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 36, 49, 64)   0           add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, 36, 49, 64)   144         dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, 36, 49, 64)   0           batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, 36, 49, 64)   0           activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_91 (Add)                    (None, 36, 49, 64)   0           activation_281[0][0]             \n",
      "                                                                 activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling2D) (None, 17, 24, 64)   0           add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 17, 24, 64)   0           max_pooling2d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 17, 24, 64)   68          dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, 17, 24, 64)   0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, 17, 24, 64)   0           activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_92 (Add)                    (None, 17, 24, 64)   0           activation_285[0][0]             \n",
      "                                                                 activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 17, 24, 64)   0           add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 17, 24, 64)   68          dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, 17, 24, 64)   0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, 17, 24, 64)   0           activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_93 (Add)                    (None, 17, 24, 64)   0           activation_289[0][0]             \n",
      "                                                                 activation_292[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 17, 24, 64)   0           add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 17, 24, 64)   68          dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 17, 24, 64)   0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 17, 24, 64)   0           activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_94 (Add)                    (None, 17, 24, 64)   0           activation_293[0][0]             \n",
      "                                                                 activation_296[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling2D) (None, 8, 11, 64)    0           add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 8, 11, 64)    0           max_pooling2d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 8, 11, 64)    32          dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 8, 11, 64)    0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 5632)         0           activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            22532       flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,408\n",
      "Trainable params: 28,070\n",
      "Non-trainable params: 2,338\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set up architecture\n",
    "stride = 1\n",
    "CHANNEL_AXIS = 1\n",
    "dropout = 0.5\n",
    "def res_layer(x ,filter1, filter2, pooling = False,dropout = 0.0):\n",
    "    temp = x\n",
    "    x = layers.Conv2D(filter1,(1,1),strides = stride,padding = \"same\")(x)\n",
    "    x = layers.BatchNormalization(axis = CHANNEL_AXIS)(temp)\n",
    "    x = layers.Activation(\"relu\")(temp)\n",
    "    \n",
    "    x = layers.Conv2D(filter1,(3,3),strides = stride,padding = \"same\")(x)\n",
    "    x = layers.BatchNormalization(axis = CHANNEL_AXIS)(temp)\n",
    "    x = layers.Activation(\"relu\")(temp)\n",
    "    \n",
    "    x = layers.Conv2D(filter2,(1,1),strides = stride,padding = \"same\")(x)\n",
    "    x = layers.BatchNormalization(axis = CHANNEL_AXIS)(temp)\n",
    "    x = layers.Activation(\"relu\")(temp)\n",
    "    \n",
    "    x = layers.Add()([temp, x])\n",
    "    if pooling:\n",
    "        x = layers.MaxPooling2D((3,3), strides=2)(x)\n",
    "    if dropout != 0.0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    x = layers.BatchNormalization(axis = CHANNEL_AXIS)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "inp = Input(shape = X_train[0].shape)\n",
    "x = inp\n",
    "x = layers.Conv2D(64, (7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization(axis = CHANNEL_AXIS)(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.MaxPooling2D((3, 3), strides=2)(x)\n",
    "x = res_layer(x, 64, 256, dropout=dropout)\n",
    "x = res_layer(x, 64, 256, dropout=dropout)\n",
    "x = res_layer(x, 64, 256, dropout=dropout, pooling=True)\n",
    "x = res_layer(x, 128, 512, dropout=dropout)\n",
    "x = res_layer(x, 128, 512, dropout=dropout)\n",
    "x = res_layer(x, 128, 512, dropout=dropout)\n",
    "x = res_layer(x, 128, 512, dropout=dropout, pooling=True)\n",
    "x = res_layer(x, 256, 1024, dropout=dropout)\n",
    "x = res_layer(x, 256, 1024, dropout=dropout)\n",
    "x = res_layer(x, 256, 1024, dropout=dropout)\n",
    "x = res_layer(x, 256, 1024, dropout=dropout)\n",
    "x = res_layer(x, 256, 1024, dropout=dropout)\n",
    "x = res_layer(x, 256, 1024, dropout=dropout, pooling=True)\n",
    "x = res_layer(x, 512, 2048, dropout=dropout)\n",
    "x = res_layer(x, 512, 2048, dropout=dropout)\n",
    "x = res_layer(x, 512, 2048, dropout=dropout, pooling=True)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(len(np.unique(y_train)), activation=\"softmax\")(x)\n",
    "resnet_model = Model(inp,x)\n",
    "resnet_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "resnet_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping and best model checkpoint\n",
    "es = EarlyStopping(monitor='accuracy', mode='max', verbose=1, min_delta=0.1, patience=40)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_weights_only=True, mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9685 - accuracy: 0.7781\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.75048, saving model to best_model.h5\n",
      "262/262 [==============================] - 62s 238ms/step - loss: 0.9685 - accuracy: 0.7781 - val_loss: 1.3799 - val_accuracy: 0.7505\n",
      "Epoch 2/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00002: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 236ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3798 - val_accuracy: 0.7505\n",
      "Epoch 3/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00003: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 236ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3797 - val_accuracy: 0.7505\n",
      "Epoch 4/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00004: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 236ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3797 - val_accuracy: 0.7505\n",
      "Epoch 5/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00005: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 239ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3796 - val_accuracy: 0.7505\n",
      "Epoch 6/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00006: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 238ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3796 - val_accuracy: 0.7505\n",
      "Epoch 7/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00007: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3796 - val_accuracy: 0.7505\n",
      "Epoch 8/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00008: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3796 - val_accuracy: 0.7505\n",
      "Epoch 9/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00009: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 236ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3795 - val_accuracy: 0.7505\n",
      "Epoch 10/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00010: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3795 - val_accuracy: 0.7505\n",
      "Epoch 11/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00011: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 236ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3795 - val_accuracy: 0.7505\n",
      "Epoch 12/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00012: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3795 - val_accuracy: 0.7505\n",
      "Epoch 13/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00013: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3795 - val_accuracy: 0.7505\n",
      "Epoch 14/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00014: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 15/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00015: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 16/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00016: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 17/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00017: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 18/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00018: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 19/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00019: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 20/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00020: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 62s 235ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 21/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00021: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 22/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00022: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 23/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00023: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3794 - val_accuracy: 0.7505\n",
      "Epoch 24/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00024: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 25/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00025: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 26/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00026: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 27/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00027: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 28/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00028: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 29/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00029: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 233ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00030: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 31/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00031: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 233ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 32/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00032: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 234ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 33/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00033: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 61s 233ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 34/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00034: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 241ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 35/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00035: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 241ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 36/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00036: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 241ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3793 - val_accuracy: 0.7505\n",
      "Epoch 37/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00037: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 240ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3792 - val_accuracy: 0.7505\n",
      "Epoch 38/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00038: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 242ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3792 - val_accuracy: 0.7505\n",
      "Epoch 39/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00039: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 240ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3792 - val_accuracy: 0.7505\n",
      "Epoch 40/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00040: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 240ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3792 - val_accuracy: 0.7505\n",
      "Epoch 41/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.9554 - accuracy: 0.7883\n",
      "Epoch 00041: val_accuracy did not improve from 0.75048\n",
      "262/262 [==============================] - 63s 240ms/step - loss: 0.9554 - accuracy: 0.7883 - val_loss: 1.3791 - val_accuracy: 0.7505\n",
      "Epoch 00041: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fitting model with early stopping and checkpoint save\n",
    "history = resnet_model.fit(X_train, y_train, epochs=50, \n",
    "                    validation_data=(X_test, y_test), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "resnet_model.load_weights('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.save(\"resnet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# save to json:  \n",
    "hist_json_file = 'resnet_history.json' \n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = resnet_model.evaluate(X_test,  y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show metrics\n",
    "predictions = resnet_model.predict([X_test])\n",
    "y_pred = predictions.argmax(axis=1).astype(int)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot train and test validation of loss values and accuracy values\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'r--', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r--', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.ylim([0,1.5])\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC binary label reshaping\n",
    "y_pred_roc = (y_pred[:,None] == np.arange(y_pred.max()+1)).astype(int)\n",
    "y_test_roc = (y_test[:,None] == np.arange(y_test.max()+1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC\n",
    "def plot_roc(y_test, y_pred):\n",
    "    # Plot linewidth.\n",
    "    lw = 2\n",
    "    n_classes = len(y_pred_roc[0])\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_pred_roc[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_pred_roc.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(1)\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc(y_test_roc,y_pred_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(matrix, index = [i for i in range(len(np.unique(y_test)))],\n",
    "                  columns = [i for i in range(len(np.unique(y_test)))])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
